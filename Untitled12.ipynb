{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN0v8+w8PiGy/KI/lYWd/RA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2403a52012-lgtm/AIAC/blob/main/Untitled12.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "uKoOxWitNNwM",
        "outputId": "8b8203f7-729e-45e0-9c8b-6b44287f0d1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[('We', 'PRP'),\n",
              "  ('propose', 'VBP'),\n",
              "  ('a', 'DT'),\n",
              "  ('novel', 'JJ'),\n",
              "  ('neural', 'JJ'),\n",
              "  ('network', 'NN'),\n",
              "  ('architecture', 'NN'),\n",
              "  ('for', 'IN'),\n",
              "  ('image', 'NN'),\n",
              "  ('classification', 'NN'),\n",
              "  ('.', '.')],\n",
              " [('This', 'DT'),\n",
              "  ('paper', 'NN'),\n",
              "  ('presents', 'VBZ'),\n",
              "  ('a', 'DT'),\n",
              "  ('probabilistic', 'JJ'),\n",
              "  ('model', 'NN'),\n",
              "  ('for', 'IN'),\n",
              "  ('sequence', 'NN'),\n",
              "  ('labeling', 'NN'),\n",
              "  ('tasks', 'NNS'),\n",
              "  ('.', '.')],\n",
              " [('Experimental', 'JJ'),\n",
              "  ('results', 'NNS'),\n",
              "  ('demonstrate', 'JJ'),\n",
              "  ('significant', 'JJ'),\n",
              "  ('improvements', 'NNS'),\n",
              "  ('over', 'IN'),\n",
              "  ('baseline', 'JJ'),\n",
              "  ('methods', 'NNS'),\n",
              "  ('.', '.')],\n",
              " [('The', 'DT'),\n",
              "  ('algorithm', 'NN'),\n",
              "  ('achieves', 'VBZ'),\n",
              "  ('state', 'NN'),\n",
              "  ('of', 'IN'),\n",
              "  ('the', 'DT'),\n",
              "  ('art', 'NN'),\n",
              "  ('performance', 'NN'),\n",
              "  ('on', 'IN'),\n",
              "  ('benchmark', 'NN'),\n",
              "  ('datasets', 'NNS'),\n",
              "  ('.', '.')],\n",
              " [('We', 'PRP'),\n",
              "  ('analyze', 'VBP'),\n",
              "  ('the', 'DT'),\n",
              "  ('convergence', 'NN'),\n",
              "  ('properties', 'NNS'),\n",
              "  ('of', 'IN'),\n",
              "  ('stochastic', 'JJ'),\n",
              "  ('gradient', 'NN'),\n",
              "  ('descent', 'NN'),\n",
              "  ('.', '.')]]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize, pos_tag\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt_tab') # Added to resolve LookupError\n",
        "nltk.download('averaged_perceptron_tagger_eng') # Added to resolve specific LookupError\n",
        "\n",
        "# Sample abstracts (shortened for lab demo)\n",
        "abstracts = [\n",
        "    \"We propose a novel neural network architecture for image classification.\",\n",
        "    \"This paper presents a probabilistic model for sequence labeling tasks.\",\n",
        "    \"Experimental results demonstrate significant improvements over baseline methods.\",\n",
        "    \"The algorithm achieves state of the art performance on benchmark datasets.\",\n",
        "    \"We analyze the convergence properties of stochastic gradient descent.\"\n",
        "]\n",
        "\n",
        "# POS tagging\n",
        "tagged_sentences = []\n",
        "for abs_text in abstracts:\n",
        "    tokens = word_tokenize(abs_text)\n",
        "    tagged = pos_tag(tokens)\n",
        "    tagged_sentences.append(tagged)\n",
        "\n",
        "tagged_sentences"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JnolIhbFNvqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "10a08d01",
        "outputId": "9922b44f-dc0e-4c91-ef52-dac7fe733ce9"
      },
      "source": [
        "transition_counts = defaultdict(lambda: defaultdict(int))\n",
        "tag_denominators = defaultdict(int)\n",
        "\n",
        "for sentence in tagged_sentences:\n",
        "    # Extract only the tags from the (word, tag) tuples\n",
        "    tags_only = [tag for word, tag in sentence]\n",
        "    # Add <START> and <END> tags\n",
        "    processed_tags = ['<START>'] + tags_only + ['<END>']\n",
        "\n",
        "    for i in range(len(processed_tags) - 1):\n",
        "        prev_tag = processed_tags[i]\n",
        "        current_tag = processed_tags[i+1]\n",
        "\n",
        "        # Increment denominator for the previous tag\n",
        "        tag_denominators[prev_tag] += 1\n",
        "        # Increment count for the (previous_tag, current_tag) pair\n",
        "        transition_counts[prev_tag][current_tag] += 1\n",
        "\n",
        "transition_probabilities = defaultdict(lambda: defaultdict(float))\n",
        "\n",
        "for prev_tag, next_tag_counts in transition_counts.items():\n",
        "    total_occurrences_of_prev_tag = tag_denominators[prev_tag]\n",
        "    for current_tag, count in next_tag_counts.items():\n",
        "        transition_probabilities[prev_tag][current_tag] = count / total_occurrences_of_prev_tag\n",
        "\n",
        "print(\"Transition Probabilities:\")\n",
        "for prev_tag, probs in transition_probabilities.items():\n",
        "    print(f\"  {prev_tag}: {probs}\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transition Probabilities:\n",
            "  <START>: defaultdict(<class 'float'>, {'PRP': 0.4, 'DT': 0.4, 'JJ': 0.2})\n",
            "  PRP: defaultdict(<class 'float'>, {'VBP': 1.0})\n",
            "  VBP: defaultdict(<class 'float'>, {'DT': 1.0})\n",
            "  DT: defaultdict(<class 'float'>, {'JJ': 0.3333333333333333, 'NN': 0.6666666666666666})\n",
            "  JJ: defaultdict(<class 'float'>, {'JJ': 0.25, 'NN': 0.375, 'NNS': 0.375})\n",
            "  NN: defaultdict(<class 'float'>, {'NN': 0.3125, 'IN': 0.25, '.': 0.125, 'VBZ': 0.125, 'NNS': 0.1875})\n",
            "  IN: defaultdict(<class 'float'>, {'NN': 0.5, 'JJ': 0.3333333333333333, 'DT': 0.16666666666666666})\n",
            "  .: defaultdict(<class 'float'>, {'<END>': 1.0})\n",
            "  VBZ: defaultdict(<class 'float'>, {'DT': 0.5, 'NN': 0.5})\n",
            "  NNS: defaultdict(<class 'float'>, {'.': 0.5, 'JJ': 0.16666666666666666, 'IN': 0.3333333333333333})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "455eedf1",
        "outputId": "4af0464e-3357-4d7f-df7b-6cd62ced9fec"
      },
      "source": [
        "emission_counts = defaultdict(lambda: defaultdict(int))\n",
        "tag_counts = defaultdict(int) # This will be the denominator for emission probabilities\n",
        "\n",
        "for sentence in tagged_sentences:\n",
        "    for word, tag in sentence:\n",
        "        emission_counts[tag][word] += 1\n",
        "        tag_counts[tag] += 1\n",
        "\n",
        "emission_probabilities = defaultdict(lambda: defaultdict(float))\n",
        "\n",
        "for tag, word_counts in emission_counts.items():\n",
        "    total_occurrences_of_tag = tag_counts[tag]\n",
        "    for word, count in word_counts.items():\n",
        "        emission_probabilities[tag][word] = count / total_occurrences_of_tag\n",
        "\n",
        "print(\"\\nEmission Probabilities:\")\n",
        "for tag, probs in emission_probabilities.items():\n",
        "    print(f\"  {tag}: {probs}\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Emission Probabilities:\n",
            "  PRP: defaultdict(<class 'float'>, {'We': 1.0})\n",
            "  VBP: defaultdict(<class 'float'>, {'propose': 0.5, 'analyze': 0.5})\n",
            "  DT: defaultdict(<class 'float'>, {'a': 0.3333333333333333, 'This': 0.16666666666666666, 'The': 0.16666666666666666, 'the': 0.3333333333333333})\n",
            "  JJ: defaultdict(<class 'float'>, {'novel': 0.125, 'neural': 0.125, 'probabilistic': 0.125, 'Experimental': 0.125, 'demonstrate': 0.125, 'significant': 0.125, 'baseline': 0.125, 'stochastic': 0.125})\n",
            "  NN: defaultdict(<class 'float'>, {'network': 0.0625, 'architecture': 0.0625, 'image': 0.0625, 'classification': 0.0625, 'paper': 0.0625, 'model': 0.0625, 'sequence': 0.0625, 'labeling': 0.0625, 'algorithm': 0.0625, 'state': 0.0625, 'art': 0.0625, 'performance': 0.0625, 'benchmark': 0.0625, 'convergence': 0.0625, 'gradient': 0.0625, 'descent': 0.0625})\n",
            "  IN: defaultdict(<class 'float'>, {'for': 0.3333333333333333, 'over': 0.16666666666666666, 'of': 0.3333333333333333, 'on': 0.16666666666666666})\n",
            "  .: defaultdict(<class 'float'>, {'.': 1.0})\n",
            "  VBZ: defaultdict(<class 'float'>, {'presents': 0.5, 'achieves': 0.5})\n",
            "  NNS: defaultdict(<class 'float'>, {'tasks': 0.16666666666666666, 'results': 0.16666666666666666, 'improvements': 0.16666666666666666, 'methods': 0.16666666666666666, 'datasets': 0.16666666666666666, 'properties': 0.16666666666666666})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transition_counts = defaultdict(Counter)\n",
        "tag_counts = Counter()\n",
        "\n",
        "for sentence in tagged_sentences:\n",
        "    prev_tag = \"<START>\"\n",
        "    for word, tag in sentence:\n",
        "        transition_counts[prev_tag][tag] += 1\n",
        "        tag_counts[prev_tag] += 1\n",
        "        prev_tag = tag\n",
        "    transition_counts[prev_tag][\"<END>\"] += 1\n",
        "    tag_counts[prev_tag] += 1\n",
        "\n",
        "# Transition probabilities\n",
        "transition_probs = {\n",
        "    prev: {tag: count / tag_counts[prev]\n",
        "           for tag, count in tags.items()}\n",
        "    for prev, tags in transition_counts.items()\n",
        "}\n",
        "\n",
        "print(\"Transition Probabilities:\")\n",
        "for prev_tag, probs in transition_probs.items():\n",
        "    print(f\"  {prev_tag}: {probs}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "o8yP3c_wOR1p",
        "outputId": "0fd343d4-cd84-44f7-d83f-1c756be9d07d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transition Probabilities:\n",
            "  <START>: {'PRP': 0.4, 'DT': 0.4, 'JJ': 0.2}\n",
            "  PRP: {'VBP': 1.0}\n",
            "  VBP: {'DT': 1.0}\n",
            "  DT: {'JJ': 0.3333333333333333, 'NN': 0.6666666666666666}\n",
            "  JJ: {'JJ': 0.25, 'NN': 0.375, 'NNS': 0.375}\n",
            "  NN: {'NN': 0.3125, 'IN': 0.25, '.': 0.125, 'VBZ': 0.125, 'NNS': 0.1875}\n",
            "  IN: {'NN': 0.5, 'JJ': 0.3333333333333333, 'DT': 0.16666666666666666}\n",
            "  .: {'<END>': 1.0}\n",
            "  VBZ: {'DT': 0.5, 'NN': 0.5}\n",
            "  NNS: {'.': 0.5, 'JJ': 0.16666666666666666, 'IN': 0.3333333333333333}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emission_counts = defaultdict(Counter)\n",
        "tag_word_counts = Counter()\n",
        "\n",
        "for sentence in tagged_sentences:\n",
        "    for word, tag in sentence:\n",
        "        emission_counts[tag][word.lower()] += 1\n",
        "        tag_word_counts[tag] += 1\n",
        "\n",
        "emission_probs = {\n",
        "    tag: {word: count / tag_word_counts[tag]\n",
        "          for word, count in words.items()}\n",
        "    for tag, words in emission_counts.items()\n",
        "}\n",
        "\n",
        "print(\"\\nEmission Probabilities:\")\n",
        "for tag, probs in emission_probs.items():\n",
        "    print(f\"  {tag}: {probs}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "WIHDovGGOoTj",
        "outputId": "3212cccf-a4ec-4e9b-e9fa-7e6ec933008c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Emission Probabilities:\n",
            "  PRP: {'we': 1.0}\n",
            "  VBP: {'propose': 0.5, 'analyze': 0.5}\n",
            "  DT: {'a': 0.3333333333333333, 'this': 0.16666666666666666, 'the': 0.5}\n",
            "  JJ: {'novel': 0.125, 'neural': 0.125, 'probabilistic': 0.125, 'experimental': 0.125, 'demonstrate': 0.125, 'significant': 0.125, 'baseline': 0.125, 'stochastic': 0.125}\n",
            "  NN: {'network': 0.0625, 'architecture': 0.0625, 'image': 0.0625, 'classification': 0.0625, 'paper': 0.0625, 'model': 0.0625, 'sequence': 0.0625, 'labeling': 0.0625, 'algorithm': 0.0625, 'state': 0.0625, 'art': 0.0625, 'performance': 0.0625, 'benchmark': 0.0625, 'convergence': 0.0625, 'gradient': 0.0625, 'descent': 0.0625}\n",
            "  IN: {'for': 0.3333333333333333, 'over': 0.16666666666666666, 'of': 0.3333333333333333, 'on': 0.16666666666666666}\n",
            "  .: {'.': 1.0}\n",
            "  VBZ: {'presents': 0.5, 'achieves': 0.5}\n",
            "  NNS: {'tasks': 0.16666666666666666, 'results': 0.16666666666666666, 'improvements': 0.16666666666666666, 'methods': 0.16666666666666666, 'datasets': 0.16666666666666666, 'properties': 0.16666666666666666}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zOGaHLs2PDuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "d0b089d9",
        "outputId": "81de773a-64b5-43fe-da14-df5e64872893"
      },
      "source": [
        "sorted_transitions = sorted(\n",
        "    [(prev, tag, prob)\n",
        "     for prev, tags in transition_probs.items()\n",
        "     for tag, prob in tags.items()],\n",
        "    key=lambda x: x[2],\n",
        "    reverse=True\n",
        ")\n",
        "\n",
        "display(sorted_transitions[:5])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[('PRP', 'VBP', 1.0),\n",
              " ('VBP', 'DT', 1.0),\n",
              " ('.', '<END>', 1.0),\n",
              " ('DT', 'NN', 0.6666666666666666),\n",
              " ('IN', 'NN', 0.5)]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9o2zHANqPQB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "078292d9",
        "outputId": "fa39cc3c-69f1-4528-91f7-871054e291f4"
      },
      "source": [
        "def viterbi(words, possible_tags, transition_probs, emission_probs):\n",
        "    # Tokenize the input sentence\n",
        "    tokens = [word.lower() for word in word_tokenize(words)]\n",
        "\n",
        "    # Initialize Viterbi path and probabilities\n",
        "    viterbi_table = defaultdict(lambda: defaultdict(float))\n",
        "    backpointer = defaultdict(lambda: defaultdict(str))\n",
        "\n",
        "    # <START> probabilities\n",
        "    for tag in possible_tags:\n",
        "        trans_prob = transition_probs.get('<START>', {}).get(tag, 0.0)\n",
        "        # If the word is unknown, assign a small probability to avoid zero\n",
        "        emit_prob = emission_probs.get(tag, {}).get(tokens[0], 1e-10) # Laplace smoothing or a small default\n",
        "        viterbi_table[0][tag] = trans_prob * emit_prob\n",
        "        backpointer[0][tag] = '<START>'\n",
        "\n",
        "    # Fill Viterbi table\n",
        "    for i in range(1, len(tokens)):\n",
        "        for current_tag in possible_tags:\n",
        "            max_prob = 0.0\n",
        "            best_prev_tag = ''\n",
        "\n",
        "            # If the word is unknown, assign a small probability to avoid zero\n",
        "            emit_prob = emission_probs.get(current_tag, {}).get(tokens[i], 1e-10) # Laplace smoothing or a small default\n",
        "\n",
        "            for prev_tag in possible_tags:\n",
        "                # Transition from prev_tag to current_tag\n",
        "                trans_prob = transition_probs.get(prev_tag, {}).get(current_tag, 0.0)\n",
        "\n",
        "                prob = viterbi_table[i-1][prev_tag] * trans_prob * emit_prob\n",
        "\n",
        "                if prob > max_prob:\n",
        "                    max_prob = prob\n",
        "                    best_prev_tag = prev_tag\n",
        "\n",
        "            viterbi_table[i][current_tag] = max_prob\n",
        "            backpointer[i][current_tag] = best_prev_tag\n",
        "\n",
        "    # Handle <END> state\n",
        "    max_prob = 0.0\n",
        "    best_last_tag = ''\n",
        "    for tag in possible_tags:\n",
        "        trans_prob = transition_probs.get(tag, {}).get('<END>', 0.0)\n",
        "        prob = viterbi_table[len(tokens) - 1][tag] * trans_prob\n",
        "        if prob > max_prob:\n",
        "            max_prob = prob\n",
        "            best_last_tag = tag\n",
        "\n",
        "    # Reconstruct the best path\n",
        "    tagged_sequence = []\n",
        "    current = best_last_tag\n",
        "    for i in range(len(tokens) - 1, -1, -1):\n",
        "        tagged_sequence.insert(0, (tokens[i], current))\n",
        "        current = backpointer[i][current]\n",
        "\n",
        "    return tagged_sequence\n",
        "\n",
        "# Get all possible tags from the training data\n",
        "possible_tags = set()\n",
        "for sentence in tagged_sentences:\n",
        "    for _, tag in sentence:\n",
        "        possible_tags.add(tag)\n",
        "\n",
        "# Run Viterbi algorithm\n",
        "predicted_tags = viterbi(input_sentence, list(possible_tags), transition_probs, emission_probs)\n",
        "\n",
        "# Format the output as requested\n",
        "output_string = \" \".join([f\"{word}/{tag.upper()}\" for word, tag in predicted_tags])\n",
        "print(f\"Output (Predicted POS Tags):\\n{output_string}\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output (Predicted POS Tags):\n",
            "the/DT proposed/NN method/VBZ improves/DT classification/NN accuracy/NNS ./.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "163d729c",
        "outputId": "ebf434e8-eadc-4759-e87e-426f7f90d243"
      },
      "source": [
        "input_sentence = \"The proposed method improves classification accuracy.\"\n",
        "print(f\"Input Sentence: {input_sentence}\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Sentence: The proposed method improves classification accuracy.\n"
          ]
        }
      ]
    }
  ]
}